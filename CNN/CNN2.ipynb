{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leochartrand/IFT615/blob/main/CNN/CNN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCdTejS5-OYw"
      },
      "source": [
        "*Cette démonstration est inspirée du cours \"CS50’s Introduction to Artificial Intelligence with Python\" donné à l'Université d'Harvard.*\n",
        "\n",
        "https://cs50.harvard.edu/ai/2024/\n",
        "\n",
        "https://creativecommons.org/licenses/by-nc-sa/4.0/\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ce notebook a pour but d'introduire les réseaux de neurones à convolution et leur développement en Python. Nous utiliserons la librairie PyTorch, qui est aujourd'hui la plus utilisée dans le développement et la recherche en apprentissage profond. Nous créerons un réseau à convolution simple afin de classifier des images de chiffres écrits à la main."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comme toujours, on commence par importer les librairies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i5YbW8fjiRD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC2_hFFuVM-b"
      },
      "source": [
        "On peut ensuite charger le jeux de données MNIST. Il s'agit d'un ensemble de 60000 images de chiffres écrits à la main. Les images sont en \"greyscale\" et d'une taille de 28x28 pixels. Le jeux de données est divisé en un ensemble d'entraînement et un ensemble de test. La raison pour ceci est que lors de l'évaluation, le modèle sera testé avec des images qu'il n'aura jamais vu durant l'entraînement, afin de vérifier ses capacités de \"généralisation\".\n",
        "\n",
        "Notez qu'on définit une variable \"batch size\". En IA, une batch est un ensembe de données sur lequel le modèle va s'entraîner en même temps. Leur usage permet théoriquement un apprentissage plus stable. En pratique, elle fait aussi utilisation des capacités de parallélisme des GPUs pour un entraînement plus rapide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjCJokIgIpo1"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root = './', train = True,\n",
        "                                        transform = torchvision.transforms.ToTensor(),\n",
        "                                        download = True)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                          batch_size = batch_size,\n",
        "                                          shuffle = True)\n",
        "\n",
        "test_data = torchvision.datasets.MNIST(root = './', train = False,\n",
        "                                       transform = torchvision.transforms.ToTensor())\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                          batch_size = batch_size,\n",
        "                                          shuffle = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lTpXTTeVT1r"
      },
      "source": [
        "Profitons-en pour observer quelques exemples du jeux de données:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fj4giPeViN_"
      },
      "outputs": [],
      "source": [
        "index, (ex_data, ex_labels) = next(enumerate(test_dataloader))\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(9):\n",
        "  plt.subplot(3,3,i+1)\n",
        "  plt.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(ex_data[i][0], cmap='gray')\n",
        "  plt.title(\"Vérité terrain: %d\"%(ex_labels[i]))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAcd5uR1XYSo"
      },
      "source": [
        "On peut maintenant instancier notre modèles et définir ses paramètres! Voici ci-dessous un exemple de réseau de neurones qui utilise une couche convolutive suivie d'une couche de pooling. Les couches convolutives sont suivies d'une couche dense cachée ainsi d'une couche dense finale qui retourne les prédictions.\n",
        "\n",
        "Pour déterminer la taille d'entrée de la couche dense cachée, il faut comprendre comment calculer la taille de sortie d'une couche convolutive. Soit la taille des données $D$ et la taille du filtre $F$, on a:\n",
        "$$D-F+1$$\n",
        "\n",
        "En pratique, les réseaux à convolution sont beaucoup plus complexes et l'équation énoncée ci-haut comporte plus de paramètres. Cependant, dans cette exemple, seuls $D$ et $F$ suffisent à déterminer les dimensions des données en sortie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Q2uuTedIQjZ2"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "\n",
        "    # Couche convolutive. Apprend 32 filtres en utilisant un noyau 3x3\n",
        "    # Arguments: \n",
        "    #   in_channels     = 1, étant donné que l'image est en greyscale. \n",
        "    #   out_channels    = 32, soit le nombre de filtres que nous voulons appliquer. \n",
        "    #   kernel_szie     = (3,3), les dimensions des filtres. \n",
        "    nn.Conv2d(1, 32, (3, 3)),   \n",
        "    # Dimensions des données de sortie: (28-3+1) = 26 -> 32 filtres en 26x26\n",
        "\n",
        "    # Activation ReLU\n",
        "    nn.ReLU(),\n",
        "\n",
        "    # Couche de max-pooling de taille 2x2\n",
        "    nn.MaxPool2d((2, 2)),       \n",
        "    # Dimensions des données de sortie: 26/2 = 13 -> 32 filtres en 13x13\n",
        "\n",
        "    # On applatie les données en un seul vecteur\n",
        "    nn.Flatten(),               \n",
        "    # Dimensions des données de sortie: 32x13x13 = 5408\n",
        "\n",
        "    # Couche dense\n",
        "    nn.Linear(5408, 128),\n",
        "    nn.ReLU(),\n",
        "\n",
        "    # Couche dense finale avec comme sortie les probabilitées pour les 10 chiffres\n",
        "    nn.Linear(128, 10),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Par principe de simplicité et de rapidité d'exécution, une seule couche convolutive a été introduite dans le réseau. Cependant, il normal d'en avoir plusieurs une à la suite de l'autre, possiblement en alternance avec des couches de pooling. À l'opposé des premières couches convolutives qui font ressortir des caractéristiques de bas niveau dans une image, les couches convolutives en fin de réseau permettent d'extraire des patrons de caractéristiques plus généraux et abstraits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le code suivant utilise la librairire PyTorch pour établir une fonction de perte ainsi qu'un algorithme d'optimisation pour la descente de gradient. Ces éléments sont tous deux responsables de l'apprentissage, tout comme avec les réseaux de neurones à couches denses. Nous ne leur porterons pas attention dans cette présentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33kLWRu1mvVw"
      },
      "outputs": [],
      "source": [
        "# On instancie un optimiseur qui s'occupe de la descente de gradient\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# On instancie notre fonction de parte\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlFHblQqoazW"
      },
      "source": [
        "# Entraînons maintenant notre modèle!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58hXKPAOJm47"
      },
      "outputs": [],
      "source": [
        "# On garde un historique de l'entraînement pour visualiser nos résultats\n",
        "loss_history = []\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(\"Epoch %d/%d\"%(epoch+1, epochs))\n",
        "  progress = tqdm(train_dataloader)\n",
        "  for index, (data, labels) in enumerate(progress):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_history.append(loss.item())\n",
        "    progress.set_description(\"Loss: %.4f\"%loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GetjAV9--3ZK"
      },
      "source": [
        "On peut visualiser l'apprentissage de notre modèle à l'aide de la librairie ***matplotlib***:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gNQ9E1o02Tx"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(epochs*len(train_dataloader)), loss_history)\n",
        "plt.xlabel('Batches')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss history')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_UoQpVE_Pni"
      },
      "source": [
        "Évaluons maintenant la précision de notre modèle sur le jeu de données de test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXEjWafLJo47"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "for data, labels in test_dataloader:\n",
        "  output = model(data)\n",
        "  _, predictions = torch.max(output,1)\n",
        "  correct += (predictions == labels).sum()\n",
        "  total += labels.size(0)\n",
        "\n",
        "print('Précision du modèle: %.3f %%' %(100*correct/total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On peut voir qu'un réseau à convolution assez simple peut offrir une performance remarquable. Si vous voulez, vous pouvez essayer d'améliorer l'architecture du réseau pour obtenir un meilleur score!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
