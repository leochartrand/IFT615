{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leochartrand/IFT615/blob/main/RNN/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ-qvLzjb91R"
      },
      "source": [
        "## Apprenons à un RNN à générer du Shakespeare!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7mtn1PSTPdt"
      },
      "source": [
        "*Mary, and will, my lord, to weep in such a one were prettiest​*\n",
        "\n",
        "​*Yet now I was adopted heir​​*\n",
        "\n",
        "​*Of the world’s lamentable day​​*\n",
        "\n",
        "​*To watch the next way with his father with his face?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dans ce notebook, nous survolerons une implémentation simple d'une réseau de neurones récurrent qui apprend, lettre par lettre, à générer du texte ressemblant à du Shakespeare. Le développement d'un RNN étant beaucoup plus complexe que pour des réseaux de neurones simples ou à convolution, plusieurs concepts ne seront pas abordés pour qu'on puisse se concentrer sur le fonctionnement général. Dans cette démonstration, vous apprendrez à quoi ressemble le traitement de données de texte afin qu'il soit interpreté par un modèle lors de son apprentissage. Nous passerons ensuite en revue une architecture de RNN simple ainsi que son entraînement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUVxZJfIeX3j"
      },
      "source": [
        "Commençons par importer nos librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "4ao0jxUWcAIP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkoSL4Cjgcis"
      },
      "source": [
        "Téléchargeons maintenant le jeu de données. Ce dernier est un ensemble de 40,000 lignes de Shakespeare provenant d'une variété de ses oeuvres. Ce *dataset* a été assemblé par Andrej Karpathy: https://github.com/karpathy/char-rnn (BSD License)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMngZsn1cBSN"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/leochartrand/IFT615/main/RNN/shakespeare.txt\n",
        "\n",
        "data = open(r'shakespeare.txt').read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMv0RFL2hWLn"
      },
      "source": [
        "Jetons un coup d'oeil au dataset. On peut imprimer par exemple les 1000 premiers charactères:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uRZGCohcDFW"
      },
      "outputs": [],
      "source": [
        "print(data[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpxvqcGVnmJY"
      },
      "source": [
        "Prétraitons le jeu de donner un peu plus. Commençons par créer une \"dictionnaire\" des caractères que l'on trouve dans le jeu de données:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqT7YEcXAcrt"
      },
      "outputs": [],
      "source": [
        "char_dict = sorted(list(set(data)))\n",
        "vocab_size = len(char_dict)\n",
        "print(\"Le jeu de données est constitué de %d caractères uniques.\"%vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INzw-l-fDCAg"
      },
      "source": [
        "Il est important que les caractères puissent être interprétés par notre modèle. Pour cela, on peut commencer par convertir chaque charactère en un indice, qui prend position dans l'intervalle [0, vocab_size[. On veut aussi pouvoir faire l'inverse et obtenir des caractères à nouveau à la sortie du modèle:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Qirpq91pDCbT"
      },
      "outputs": [],
      "source": [
        "# Pour éviter de faire la conversion plusieurs fois et de répéter du code, on\n",
        "# peut créer des dictionnaires qui font un \"mapping\" entre les caractères et leur index\n",
        "char_to_index = {char:i for i,char in enumerate(char_dict)}\n",
        "index_to_char = {i:char for i,char in enumerate(char_dict)}\n",
        "\n",
        "# Créons une simple fonction qui permettra d'obtenir les caractères directement lors de la génération:\n",
        "def get_char(index):\n",
        "  return index_to_char[index]\n",
        "def get_index(char):\n",
        "  return char_to_index[char]\n",
        "\n",
        "# On peut déjà convertir notre jeu de données en une liste d'indices!\n",
        "data = list(data)\n",
        "for i, char in enumerate(data):\n",
        "    data[i] = get_index(char)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDP5Q6n1AhBo"
      },
      "source": [
        "On peut maintenant créer notre modèle! On peut voir ici que le modèle comporte en fait 3 sous-modules:\n",
        " - Une couche de \"embedding\": Cette couche permet de projeter les caractères dans un espace latent où les relations entre ces derniers peuvent être établies selon leur position dans l'espace. La sortie de cette couche est donc un vecteur.\n",
        " - Une couche RNN: Cette couche prend en entrée le vecteur de caractère et ainsi qu'un état caché (aussi un vecteur) qui représente tous les caractères ayant été vu avant celui-ci. La sortie de cette couche est un état caché mis à jour ainsi qu'un vecteur de sortie.\n",
        " - Une couche dense finale qui permet la classification. Elle prend en entrée le vecteur de prédiction sortant du RNN et le transforme en vecteur de prédictions pour chaque caractère.\n",
        "\n",
        " Mais lorsqu'on lit la première lettre, d'où vient l'état caché? En fait, il suffit de donner un tensor nul (à zéro), qui permettra de prendre que le caractère dans l'équation à ce moment. C'est à cela que sert la fonction init_hidden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "uSg5pHljcFkb"
      },
      "outputs": [],
      "source": [
        "class Char_RNN(nn.Module):\n",
        "  def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
        "    super(Char_RNN, self).__init__()\n",
        "    self.layers = num_layers\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, input_size)\n",
        "    self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "    self.dense = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    embedded = self.embedding(input)\n",
        "    output, hidden = self.rnn(embedded, hidden)\n",
        "    output = self.dense(output)\n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.layers, batch_size, self.hidden_size)\n",
        "\n",
        "char_model = Char_RNN(vocab_size, vocab_size, 128, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCHaJKHlUlrq"
      },
      "source": [
        "On passe à l'entraînement... Dans ce bloc de code, il y a beaucoup d'opérations qui ne seront pas expliquées dans le cadre de ce cours. Ce qu'il faut retenir, c'est que les données de texte sont de nature linéaire et de taille variable. Cela requiert un prétraitement de données un peu plus élaboré que ce qu'on a vu avec d'autres types de réseau.\n",
        "\n",
        "Lors de l'entrainement, les données sont présentées en séquences d'un longueur prédéterminée. Le modèle prédit le caractère qui suit la séquence et compare avec la vérité terrain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paKNK5TAcKJz"
      },
      "outputs": [],
      "source": [
        "# Définissons nos hyperparametres\n",
        "epochs = 5\n",
        "batch_size = 100\n",
        "sequence_length = 50\n",
        "learning_rate = 5e-4\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(char_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# On garde un historique de l'entraînement pour visualiser nos résultats\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    total_batches = (len(data) - sequence_length) // (sequence_length * batch_size) # nombre de batches au total\n",
        "    with tqdm(total=total_batches) as progress:\n",
        "        for _ in range(total_batches):\n",
        "            start_index = random.randint(0, 100)\n",
        "\n",
        "            # Détecte si on est rendu à la dernière batch de l'epoch\n",
        "            if start_index + sequence_length * batch_size > len(data) - sequence_length:\n",
        "                continue\n",
        "\n",
        "            # Génère les séquences utilisées comme entrées et les séquences cibles (un caractère plus loin)\n",
        "            input_seq = torch.tensor([data[j:j+sequence_length] for j in range(start_index, start_index + sequence_length * batch_size, sequence_length)])\n",
        "            target_seq = torch.tensor([data[j+1:j+sequence_length+1] for j in range(start_index, start_index + sequence_length * batch_size, sequence_length)])\n",
        "\n",
        "            if input_seq.size(0) != batch_size:\n",
        "                continue\n",
        "\n",
        "            # Initialise l'état caché et prépare le modèle\n",
        "            hidden = char_model.init_hidden(batch_size)\n",
        "            char_model.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output, hidden = char_model(input_seq, hidden)\n",
        "            # Calcule de la perte\n",
        "            loss = criterion(output.view(-1, vocab_size), target_seq.view(-1))\n",
        "\n",
        "            # Rétropropagation des gradients\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Pour visualisation de l'apprentissage\n",
        "            loss_history.append(loss.item())\n",
        "            progress.update(1)\n",
        "            progress.set_description(\"Loss: %.4f\"%loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nji4nHPzKCuI"
      },
      "source": [
        "Visualisons la courbe d'apprentissage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jor1v86SAKi3"
      },
      "outputs": [],
      "source": [
        "plt.plot(loss_history)\n",
        "plt.xlabel('Batches')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss history')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8H-B50aKl0V"
      },
      "source": [
        "### Essayons maintenant degénérer du Shakespeare!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myCWATricMun"
      },
      "outputs": [],
      "source": [
        "seed = \"Bob\"        # Bout de texte à partir duquel le modèle commence la génération\n",
        "temperature = 0.9   # Hyperparamètre qui permet une stochasticité de génération, i.e. ne pas toujours obtenir le même texte\n",
        "gen_length = 2000   # Longueur du texte généré (en caractères)\n",
        "\n",
        "char_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for _ in range(gen_length):\n",
        "    # On prépare les données et le modèle\n",
        "    input_tensor = torch.tensor([[get_index(c) for c in seed][-sequence_length:]], dtype=torch.long)\n",
        "    hidden = char_model.init_hidden(1)\n",
        "\n",
        "    # Forward pass\n",
        "    output, hidden = char_model(input_tensor, hidden)\n",
        "\n",
        "    # On sélectionne le charactère selon les probabilités (sous forme d'indice)\n",
        "    # Ici, une variable de température assure une certaine stochasticité\n",
        "    logits = output[0, -1, :] / temperature\n",
        "    # On normalise les probabilités avec un softmax\n",
        "    prob_dist = F.softmax(logits, dim=0)\n",
        "\n",
        "    # On échantillone ensuite un indice dans la distribution\n",
        "    top_char_index = torch.multinomial(prob_dist, 1).item()\n",
        "\n",
        "    # On reconvertit en charactère\n",
        "    predicted_char = get_char(top_char_index)\n",
        "    seed += predicted_char\n",
        "\n",
        "    # Print the new character\n",
        "    print(predicted_char, end='', flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On peut voir qu'en très peut de temps, un modèle RNN peut apprendre à générér du texte relativement vraisemblable et qui peut adopter un style particulier. Remarquez qu'ici, le modèle travaille avec les caractères plutôt qu'avec les mots. Si on avait choisi ces derniers, cela aurait plusieurs effets. Premièrement, le vocabulaire deviendrait immense et donc de même que pour la couche d'embedding. Cependant, la cohérence des phrases serait bien meilleure étant donné que le modèle établirait une relation entre les mots directement. En général, un modèle de langage travaille surtout avec les mots."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNZOdEdKhFJFNCxPl84Ly4W",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
